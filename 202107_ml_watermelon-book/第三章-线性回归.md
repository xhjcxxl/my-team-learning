# 第三章

## 线性回归

### 线性模型

主要是对属性进行线性组合来建模，学习出一个函数，然后用这个函数来进行预测：

$$f(x) = w_1x_1 + w_2x_2 + ... + w_dx_d + b = \sum_{i=1}^m(w_ix_i)$$

向量形式：

$$f(X) = W^TX + b$$

每一个w都表示一个属性的权重。

线性模型有很好的解释性，每一个w都是权重，都可以有对应的解释：

$$f_{好瓜}(x)=0.2 \cdot x_{色泽} + 0.5 \cdot x_{根蒂} + 0.3 \cdot x_{敲声} + 1$$

显然 w就是这里面的小数，对各种属性进行加权，最终线性组合出一个结果，表示好瓜的得分。

### 损失函数

使用均方误差函数来表示衡量 **预测结果和真实结果之间的差距**：

$$E_{(w, b)} = \sum_{i=1}^m(f(x_i) - y_i)^2 = \sum_{i=1}^m(wx_i + b - y_i)^2$$

我们的目的是让：真实结果和预测结果之间的差距最小，需要求解出$w和b$使得这个均方误差最小，这个求解方法就是 **最小二乘法**：

直接对$w$求偏导：

$$ \frac{\partial E_{(w,b)}}{\partial w} = 2\sum_{i=1}^m(w x_i + b - y_i) (w x_i + b)^{'} \\
= 2\sum_{i=1}^m(w x_i + b - y_i) x_i \\
= 2 ( w \sum_{i=1}^m x_i^2 + \sum_{i=1}^m (b - y_i) x_i ) $$

对$b$求偏导:

$$ \frac{\partial E_{(w,b)}}{\partial b} = 2\sum_{i=1}^m(w x_i + b - y_i)\cdot 1 \\
= 2{\sum_{i=1}^m b + \sum_{i=1}^m(w x_i - y_i)} \\
= 2mb + 2 \sum_{i=1}^m(w x_i - y_i) $$

求最优解，则令 $\frac{\partial E_{(w,b)}}{\partial w} = 0$，$\frac{\partial E_{(w,b)}}{\partial b} = 0$，则：

$$ 2 ( w \sum_{i=1}^m x_i^2 + \sum_{i=1}^m (b - y_i) x_i ) = 0 \\ => \\
w\sum_{i=1}^m x_i^2 + \sum_{i=1}^m (b - y_i) x_i = 0$$

$$ 2mb + 2 \sum_{i=1}^m(w x_i - y_i) = 0 \\ => \\
b = -\frac{1}{m} \sum_{i=1}^m(w x_i - y_i)$$

把 $b$ 带入 上面的方程：

$$ w\sum_{i=1}^m x_i^2 + \sum_{i=1}^m (-\frac{1}{m} \sum_{i=1}^m(w x_i - y_i) - y_i) x_i ) = 0 $$

把后面拆开：

$$ w\sum_{i=1}^m x_i^2 + \sum_{i=1}^m x_i (-\frac{1}{m} \sum_{i=1}^m(w x_i - y_i)) - \sum_{i=1}^m y_i x_i = 0 $$

$$ w\sum_{i=1}^m x_i^2 -\frac{1}{m} \sum_{i=1}^m x_i \sum_{i=1}^m w x_i + \frac{1}{m}\sum_{i=1}^m y_i - \sum_{i=1}^m y_i x_i  = 0 $$

$$ w\sum_{i=1}^m x_i^2 -\frac{w}{m} \sum_{i=1}^m x_i \sum_{i=1}^m x_i + \frac{1}{m} \sum_{i=1}^m x_i \sum_{i=1}^m y_i - \sum_{i=1}^m y_i x_i  = 0 $$

因为 $\frac{1}{m} \sum_{i=1}^m x_i = \bar x$， 则：

$$ w\sum_{i=1}^m x_i^2 -\frac{w}{m} (\sum_{i=1}^m x_i)^2 + \bar x\sum_{i=1}^m y_i - \sum_{i=1}^m y_i x_i  = 0 $$

$$ w(\sum_{i=1}^m x_i^2 -\frac{1}{m} (\sum_{i=1}^m x_i)^2) + \bar x\sum_{i=1}^m y_i - \sum_{i=1}^m y_i x_i  = 0 $$

$$ w = \frac{\sum_{i=1}^m y_i x_i - \bar x\sum_{i=1}^m y_i}{\sum_{i=1}^m x_i^2 -\frac{1}{m} (\sum_{i=1}^m x_i)^2}$$

$$ w = \frac{\sum_{i=1}^m y_i(x_i - \bar x)}{\sum_{i=1}^m x_i^2 -\frac{1}{m} (\sum_{i=1}^m x_i)^2}$$

把解得的 $w$ 带回 $b$，求解 $b$

## 对数几率回归（逻辑回归 LR）

正常情况下，我们的线性回归方程，意思为 **$y$ 成线性**：

$$y = W^TX + b$$

如果，我们的 **$y$的对数形式，成线性**，那么有：

$$ \ln y = W^TX + b$$

我们就把这种形式叫做：**对数线性回归**，而 $y = e^{W^T X + b}$ 的结果表示 为指数形式，是看不出线性规律的，但是如果我们对其 取对数，**$\ln y$ 就变成线性**了

如果我们有一个**单调可微**的函数，专门用来代替这个指数$e$，有：

$$y = g^{-1}(W^TX + b)$$

我们把这一类线性归回叫：**广义线性回归**

### sigmoid函数

如果我们想要用线性回归来进行分类，可以在广义线性模型中，用一个函数，对 y的结果进行判断，小于多少让函数 $g^{-1}$为0，大于多少为1

常规的单位阶跃函数，无法连续，也无法可微，所以我们选择一个单调可微的函数，叫：$sigmoid$，这个函数也叫 **逻辑函数**：

$$f(z) = \frac{1}{1 + e^{-z}}$$

带入到**广义线性回归方程**中：

$$y = \frac{1}{1 + e^{-(W^T X + b)}}$$

转换形式为：

$$\ln \frac{y}{1-y} = W^T X + b $$

可以解释为：**$y 和 1-y$比值的对数 呈线性**， $y$为正例，$1-y$为反例，也就是 **正例 和 反例 的比值 的对数 呈线性**，这个叫做：**对数几率**

所以这个广义线性回归 也叫：**对数几率回归**

因为用的函数是逻辑函数：$sigmoid$函数，所以也叫：**逻辑回归**

### 损失函数

因为 $y$ 为 正例，看成后验概率：$p(y=1|x)$， $1-y$ 为反例，所以有：

$$\ln \frac{p(y=1|x)}{1 - p(y=1|x)} = \ln \frac{p(y=1|x)}{p(y=0|x)} = W^T X + b $$

则有：

$$ p(y=1|x) = \frac{e^{W^T X + b}}{1 + e^{W^T X + b}} = \frac{1}{1 + e^{-(W^T X + b)}} $$

$$ p(y=0|x) = \frac{1}{1 + e^{W^T X + b}} $$

那么我们的目的是让模型尽可能的预测正确，是正例的预测为正，反例的预测为负，所以使用最大似然法来求解：

极大似然估计：**让所有的样本的概率乘积最大化**

$$ L(w,b) = \prod_{i=1}^m P(y_i|x_i;w, b) $$

令：$ \theta = (w,b)，W^T x + b = \theta^T x $

$p_1(x;\theta) = p(y=1|x;\theta)，p_0(x;\theta) = p(y=0|x;\theta)$

则有：

$$ P(y_i|x_i;w, b) = p_1(x;\theta)^{y_i} \cdot p_0(x;\theta)^{(1 - y_i)}$$

带回式子可得：

$$L(\theta) = \prod^m_{i=1} P(y_i|x_i; \theta) = \prod^m_{i=1} p_1(x;\theta)^{y_i} \cdot (1 - p_1(x;\theta))^{(1 - y_i)}$$

其中 m 为样本个数，连乘符号表示所有的概率乘积到一起

使用对数化简为：

$$l(\theta) = \log L(\theta) = \sum^m_{i=1} [y_i \log p_1(x;\theta) + (1 - y_i) \log (1 - p_1(x;\theta))]$$

极大似然估计就是 $\theta$ 的所有可能取值中找到一个值，使得极大似然函数取到最大值

即 $maxl(\theta)$，通过添加一个系数 $-\frac{1}{m}$，将求最大值转换为求最小值，然后就可以使用梯度下降了：

$$min J(\theta) = - \frac{1}{m} l(\theta)$$

求偏导为：

$$\frac{\partial}{\partial \theta_j} J(\theta) = - \frac{1}{m} \sum^m_{i=1} [y_i \frac{1}{p_1(x;\theta)} - (1 - y_i) \frac{1}{(1 - p_1(x;\theta))}] \frac{\partial}{\partial \theta_j} p_1(x;\theta)$$

整理后得到：

$$\frac{\partial}{\partial \theta_j} J(\theta) = - \frac{1}{m} \sum^m_{i=1} (y_i - p_1(x;\theta)) x^j_i$$

$p_1(x;\theta)$ 就是sigmiod函数的逻辑回归函数， $x^j_i$表示的是第i个样本的 第j个特征

## 线性判别分析（LDA）

线性判别分析：给定训练样本，想办法把训练样本投影到一条直线上，使得 **同类样本之间的距离尽可能近**，**不同类的样本之间的距离尽可能的远**

如果输入新样本，将其投影到直线上，然后根据投影到的位置来进行判断这个样本的类别。

给定数据集：$D = {(x_i, y_i)}^m_{i=1}, 类别：y_i \in ({0, 1})$，并且设置 $X_i$ 为 第 $i$ 类样本的集合， $\mu_i$ 为 均值向量， $\sum_i$ 为 协方差矩阵，那么有：

- 将样本数据投影到直线 $w$ 上，则 两个类别的中心在直线上的投影就是 均值向量乘以对应的直线，就是在直线上的映射：$w^T \mu_0$ 和 $w^T \mu_1$

- 将所有的样本数据投影到直线上，那么就会得到两个类别样本的协方差矩阵 $w^T \sum_0 w$ 和 $w^T \sum_1 w$

所以：

- 同类别的样本投影之间的距离尽可能的近，那么 同类样本的协方差就要尽可能地小，$w^T \sum_0 w + w^T \sum_1 w$ 尽可能小

- 不同类样本之间的距离尽可能远，即不同类别之间的中心尽可能的远，$\parallel w^T \mu_0 - w^T \mu_1 \parallel_2^2$ 尽可能大

同时考虑两者，**最大化目标函数**：

$$J = \frac{\parallel w^T \mu_0 - w^T \mu_1 \parallel_2^2}{w^T \sum_0 w + w^T \sum_1 w} \\
= \frac{w^T (\mu_0 - \mu_1)(\mu_0 - \mu_1)^T w}{w^T (\sum_0 + \sum_1) w} $$

**类内散度矩阵**：

$$S_w = \sum_0 + \sum_1 \\ 
= \sum_{x \in X_0} (x - \mu_0)(x - \mu_0)^T + \sum_{x \in X_1} (x - \mu_1)(x - \mu_1)^T $$

**类间散度矩阵**：

$$S_b = (\mu_0 - \mu_1)(\mu_0 - \mu_1)^T$$

所以重写为：

$$J = \frac{w^T S_b w}{w^T S_w w} $$

LDA的最大化目标，$S_b$ 与 $S_w$ 的 **广义瑞利商**

## 多分类学习

可以将 二分类转为 到多分类的应用：

- 一对余，二分类，其他类别归为一类

- 一对一：没两个类别之间进行分类，需要训练 n(n-1)/2 个分类器，然后新样本的时候，带入所有的分类器中然后进行投票

- 多对多：直接对多对多进行建模，直接进行多分类。使用softmax 获取 对应的类别的概率，然后最大概率的为最终答案。

