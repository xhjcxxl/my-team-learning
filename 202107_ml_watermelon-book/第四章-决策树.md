# 第四章-决策树

1.一个具体事件的信息量应该是随着其发生概率而递减的，且不能为负。就是说，越具体，越容易确定的事包含的信息越少，因为大家都知道了

信息量公式： $h(x) = - \log_2{p(x)}$  用来衡量某个事件的信息量

简单来说 信息的大小跟随机事件的概率有关。**越小概率的事情发生了产生的信息量越大**

2.具体的衡量标准就是 **信息度量**

信息量度量的是**一个具体事件发生了所带来的信息**，而**熵则是在结果出来之前对可能产生的信息量的期望**——**考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望**

这个其实是期望公式：

信息熵（信息的期望）公式：

$$H(X) = - \sum^n_{i=1}{p(x_i) \log{p(x_i)}}$$

其中 $p(x_i)$ 代表随机事件X为 $x_i$ 的概率，

这里我再说一个对信息熵的理解。信息熵还可以作为一个系统复杂程度的度量，如果**系统越复杂**，出现**不同情况的种类越多**，那么他的**信息熵是比较大的**

## ID3

ID3 算法是建立在奥卡姆剃刀（用较少的东西，同样可以做好事情）的基础上：**越是小型的决策树越优于大的决策树**

### 思想

从信息论的知识中我们知道：**信息熵越大，从而样本纯度越低（越不容易确定，所以信息很大）**。

ID3 算法的核心思想就是**以信息增益来度量特征选择**，**选择信息增益最大的特征进行分裂**。算法采用**自顶向下的贪婪搜索**遍历可能的决策树空间，大致步骤为：

    1.初始化特征集合和数据集合；
    
    2.计算数据集合信息熵和所有特征的条件熵，选择信息增益最大的特征作为当前决策节点；
    
    3.更新数据集合和特征集合（删除上一步使用的特征，并按照特征值来划分不同分支的数据集合）；
    
    4.重复 2，3 两步，若子集值包含单一特征，则为分支叶子节点。

### 划分标准

ID3 使用的分类标准是信息增益，它表示**得知特征 A 的信息**而**使得样本集合不确定性减少的程度**。 如果得知一个特征的信息，能够很大部分把样本能够分成更加清楚的类别，就是**不确定度减少最多**。比如说如果我知道 青皮和红皮能够非常大的程度确定西瓜好不好吃，那么我应该首先用这个判断来判断好不好吃

数据集的信息熵：

$$H(D) = - \sum^K_{k=1}{\frac{|C_k|}{|D|} \log_2{\frac{|C_k|}{|D|}}}$$

其中 $C_k$ 表示集合 D 中属于第 k 类样本的样本子集

($C_k / D$就是 样本属于k类的概率，就是 这个集合中的k类别的 样本个数 除以 总个数)

针对某个特征 A，对于数据集 D 的条件熵 $H(D|A)$ 为:

$$H(D|A) = \sum^n_{i=1} {\frac{|D_i|}{|D|} H(D_i)} = - \sum^n_{i=1} {\frac{|D_i|}{|D|} (\sum^K_{k=1}{\frac{|D_{ik}|}{|D_i|} \log_2{\frac{|D_{ik}|}{|D_i|}}})}$$

其中 $D_i$ 表示 D 中特征 A 取第 i 个值的样本子集， $D_{ik}$ 表示 $D_i$ 中属于第 k 类的样本子集。

信息增益 = 信息熵 - 条件熵:

$$Gain(D, A) = H(D) - H(D|A)$$

信息增益越大表示使用特征 A 来划分所获得的“**纯度提升越大**”，(就是说 **在不知道特征A的情况下的信息熵和知道A条件下的信息熵之间的差别有多大**)

### 缺点

ID3:

    没有剪枝策略，容易过拟合；
    信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1；
    只能用于处理离散分布的特征；
    没有考虑缺失值。

## C4.5

C4.5 算法最大的特点是克服了 ID3 对**特征数目的偏重这一缺点**，引入信息增益率来作为分类标准。

### 思想

C4.5 相对于 ID3 的缺点对应有以下改进方式：

- 引入悲观剪枝策略进行后剪枝；
- 引入信息增益率作为划分标准；
- 将连续特征离散化，假设 n 个样本的连续特征 A 有 m 个取值，**C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点**；

对于缺失值的处理可以分为两个子问题：

问题一：在特征值缺失的情况下进行划分特征的选择？（即如何计算特征的信息增益率）
问题二：选定该划分特征，对于缺失该特征值的样本如何处理？（即到底把这个样本划分到哪个结点里）
针对问题一，C4.5 的做法是：**对于具有缺失值特征，用没有缺失的样本子集所占比重来折算**；
针对问题二，C4.5 的做法是：**将样本同时划分到所有子节点，不过要调整样本的权重值，其实也就是以不同概率划分到不同节点中**。

### 划分标准

利用信息增益率可以克服信息增益的缺点，其公式为：

$$Gain_{ratio}(D, A) = \frac{Gain(D, A)}{H_A(D)}$$

$$H_A(D) = \sum^n_{i=1}{\frac{|D_i|}{|D|} \log_2{\frac{|D_i|}{|D|}}}$$

$H_A(D)$ 称为特征 A 的固有值

信息增益率对**可取值较少的特征有所偏好**（分母越小，整体越大），因此 C4.5 并不是直接用增益率最大的特征进行划分，而是使用一个启发式方法：**先从候选划分特征中找到信息增益高于平均值的特征，再从中选择增益率最高的**

### 剪枝策略

为什么要剪枝：**过拟合的树在泛化能力的表现非常差**

#### 预剪枝

在节点划分前来确定是否继续增长，及早停止增长的主要方法有：

- 节点内数据样本低于某一阈值；

- 所有节点特征都已分裂；

- **节点划分前准确率比划分后准确率高**。

- 预剪枝不仅可以**降低过拟合的风险而且还可以减少训练时间**，但另一方面它是基于“贪心”策略，**会带来欠拟合风险**

#### 后剪枝

在已经生成的决策树上进行剪枝，从而得到**简化版的剪枝决策树**。

C4.5 采用的悲观剪枝方法，用**递归的方式从低往上针对每一个非叶子节点，评估用一个最佳叶子节点去代替这课子树是否有益**。如果**剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉**。C4.5 通过训练数据集上的错误分类数量来估算未知样本上的错误率。

**后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树**。但同时其**训练时间会大的多**。

#### 缺点

剪枝策略可以再优化；

- C4.5 用的是多叉树，用二叉树效率更高；
- C4.5 只能用于分类；
- C4.5 使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算；
- C4.5 在构造树的过程中，对**数值属性值需要按照其大小进行排序，从中选择一个分割点**，所以**只适合于能够驻留于内存的数据集**，当训练集大得无法在内存容纳时，程序无法运行。

## CART

ID3 和 C4.5 虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但是**其生成的决策树分支、规模都比较大**，CART 算法的**二分法可以简化决策树的规模，提高生成决策树的效率**。

### 思想

CART 包含的基本过程有**分裂，剪枝和树选择**。

分裂：分裂过程是一个二叉递归划分过程，其输入和预测特征既可以是连续型的也可以是离散型的，CART 没有停止准则，会一直生长下去；

剪枝：采用**代价复杂度剪枝**，从最大树开始，每次**选择训练数据熵对整体性能贡献最小的那个分裂节点作为下一个剪枝对象**，直到只剩下根节点。CART 会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树；

树选择：**用单独的测试集评估每棵剪枝树的预测性能**（也可以用交叉验证）。

CART 在 C4.5 的基础上进行了很多提升:

- C4.5 为**多叉树，运算速度慢，CART 为二叉树，运算速度快**；
- C4.5 **只能分类，CART 既可以分类也可以回归**；
- CART **使用 Gini 系数作为变量的不纯度量，减少了大量的对数运算**；
- CART **采用代理测试来估计缺失值**，而 C4.5 以不同概率划分到不同节点中；
- CART 采用“**基于代价复杂度剪枝**”方法进行剪枝，而 C4.5 采用悲观剪枝方法。

### 划分标准

**熵模型拥有大量耗时的对数运算**，基尼指数在简化模型的同时还保留了熵模型的优点。基尼指数代表了**模型的不纯度，基尼系数越小，不纯度越低，特征越好**。这和信息增益（率）正好相反

$$Gini(D) = \sum^K_{k=1}{\frac{|C_k|}{|D|} (1 - {\frac{|C_k|}{|D|}})} = 1 - \sum^K_{k=1}(\frac{|C_k|}{|D|})^2$$

$$Gini(D|A) = \sum^n_{i=1}{\frac{|D_i|}{|D|} Gini(D_i)}$$

其中 k 代表类别。

基尼指数**反映了从数据集中随机抽取两个样本，其类别标记不一致的概率**。因此**基尼指数越小，则数据集纯度越高**。基尼指数偏向于特征值较多的特征，类似信息增益。基尼指数可以用来度量任何不均匀分布，是介于 0~1 之间的数，0 是完全相等，1 是完全不相等，

此外，当 CART 为二分类，其表达式为:

$$Gini(D|A) = {\frac{|D_1|}{|D|} Gini(D_1)} + {\frac{|D_2|}{|D|} Gini(D_2)}$$

我们可以看到在平方运算和二分类的情况下，其运算更加简单。当然其性能也与熵模型非常接近

### 缺失值处理

上文说到，模型对于缺失值的处理会分为两个子问题：

**如何在特征值缺失的情况下进行划分特征的选择**？

**选定该划分特征，模型对于缺失该特征值的样本该进行怎样处理**？

对于问题 1，CART 一开始严格要求分裂特征评估时**只能使用在该特征上没有缺失值的那部分数据**，在后续版本中，CART 算法使用了一种惩罚机制来抑制提升值，从而反映出缺失值的影响（例如，如果一个特征在节点的 20% 的记录是缺失的，那么这个特征就会减少 20% 或者其他数值）

对于问题 2，CART 算法的机制是**为树的每个节点都找到代理分裂器，无论在训练数据上得到的树是否有缺失值都会这样做**。在代理分裂器中，特征的分值必须超过默认规则的性能才有资格作为代理（即代理就是代替缺失值特征作为划分特征的特征），当 CART 树中遇到缺失值时，这个实例划分到左边还是右边是决定于其排名最高的代理，如果这个代理的值也缺失了，那么就使用排名第二的代理，以此类推，如果所有代理值都缺失，那么默认规则就是把样本划分到较大的那个子节点。代理分裂器可以确保无缺失训练数据上得到的树可以用来处理包含确实值的新数据。

### 剪枝策略

采用一种“基于代价复杂度的剪枝”方法进行后剪枝，**这种方法会生成一系列树，每个树都是通过将前面的树的某个或某些子树替换成一个叶节点而得到的，这一系列树中的最后一棵树仅含一个用来预测类别的叶节点**。然后用一种成本复杂度的度量准则来判断哪棵子树应该被一个预测类别值的叶节点所代替。这种方法需要使用一个单独的测试数据集来评估所有的树，根据它们在测试数据集熵的分类性能选出最佳的树。

### 类别不平衡

CART 的一大优势在于：**无论训练数据集有多失衡，它都可以将其子冻消除不需要建模人员采取其他操作。**

CART 使用了一种先验机制，其作用相当于对类别进行加权。这种先验机制嵌入于 CART 算法判断分裂优劣的运算里，在 CART 默认的分类模式中，总是要计算每个节点关于根节点的类别频率的比值，这就相当于对数据自动重加权，对类别进行均衡。

## 总结

最后通过总结的方式对比下 ID3、C4.5 和 CART 三者之间的差异。

除了之前列出来的划分标准、剪枝策略、连续值确实值处理方式等之外，我再介绍一些其他差异：

划分标准的差异：**ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征**。

使用场景的差异：**ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；**

样本数据的差异：**ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大**

样本特征的差异：**ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征**；

剪枝策略的差异：**ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝**。
